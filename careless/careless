#!/usr/bin/env python

from careless.parser import parser
# This needs to be called before tf is imported 
# otherwise the log level will not be set properly
parser = parser.parse_args()

from careless.merge.merge import *
import numpy as np

def main(merger, parser):
    if parser.prior_mtz:
        merger.append_reference_data(parser.prior_mtz)

    if parser.separate_files:
        merger.prep_indices(separate_files=True, image_id_key=parser.image_id_key)
    else:
        merger.prep_indices(separate_files=False, image_id_key=parser.image_id_key)

# Now populate merger.prior
    if parser.normal_prior:
        merger.add_normal_prior()
    elif parser.rice_prior:
        merger.add_rice_prior()
    elif parser.laplace_prior:
        merger.add_laplace_prior()
    elif parser.studentt_prior_dof:
        merger.add_studentt_prior(parser.studentt_prior_dof)
    else:
        merger.add_wilson_prior()

    if parser.rice_woolfson_surrogate:
        merger.add_rice_woolfson_posterior()

#Now populate merger.likelihood
    if parser.quadrature_points is not None:
        s = parser.quadrature_points
        if parser.laplace_likelihood:
            merger.add_laplace_quad_likelihood()
        elif parser.studentt_likelihood_dof:
            merger.add_studentt_quad_likelihood(parser.studentt_likelihood_dof)
        else:
            merger.add_normal_quad_likelihood()
    else:
        s = parser.mc_samples
        if parser.laplace_likelihood:
            merger.add_laplace_likelihood()
        elif parser.studentt_likelihood_dof:
            merger.add_studentt_likelihood(parser.studentt_likelihood_dof, parser.studentt_scale)
        else:
            merger.add_normal_likelihood()

    metadata_keys = parser.metadata_keys.split(',')
    merger.add_scaling_model(parser.sequential_layers, metadata_keys)

    losses = merger.train_model(
        parser.iterations, 
        mc_samples=parser.mc_samples, 
        learning_rate=parser.learning_rate,
        beta_1=parser.beta_1,
        beta_2=parser.beta_2,
        clip_value=parser.clip_value,
    )
    np.save(f"{parser.output_base}_losses.npy", losses)

    for file_id,df in merger.results.groupby("experiment_id"):
        #df = df.dropna() #This will not be necessary after i fix "rescue_variational_distributions"
        df.cell,df.spacegroup = merger.results.cell,merger.results.spacegroup
        del(df['experiment_id'])
        if parser.anomalous:
            df.merged = True
            df = df.unstack_anomalous()[["F(+)", "SigF(+)", "F(-)", "SigF(-)", "N(+)", "N(-)"]]
            centrics = df.label_centrics().CENTRIC
            df.loc[centrics, 'F(-)'] = df.loc[centrics, 'F(+)']
            df.loc[centrics, 'SigF(-)'] = df.loc[centrics, 'SigF(+)']
            df.loc[centrics, 'N(-)'] = df.loc[centrics, 'N(+)']
        df.fillna(0., inplace=True)
        df = df.reset_index().infer_mtz_dtypes().set_index(['H', 'K', 'L'])
        df.write_mtz(f"{parser.output_base}_{file_id}.mtz")
    return merger


if parser.type == 'mono':
    merger = MonoMerger.from_isomorphous_mtzs(
        *parser.mtzinput, 
        anomalous=parser.anomalous, 
        dmin=parser.dmin, 
        isigi_cutoff=parser.isigi_cutoff, 
        intensity_key=parser.intensity_key
    )
    half1,half2 = MonoMerger.half_datasets_from_isomorphous_mtzs(
        *parser.mtzinput, 
        anomalous=parser.anomalous, 
        dmin=parser.dmin, 
        isigi_cutoff=parser.isigi_cutoff, 
        intensity_key=parser.intensity_key
    )
    merger.append_z_score_metadata(separate_files=parser.separate_files)
    half1.append_z_score_metadata(separate_files=parser.separate_files)
    half2.append_z_score_metadata(separate_files=parser.separate_files)
elif parser.type == 'poly':
    merger = PolyMerger.from_isomorphous_mtzs(
        *parser.mtzinput, 
        anomalous=parser.anomalous, 
        dmin=None, 
        isigi_cutoff=parser.isigi_cutoff, 
        intensity_key=parser.intensity_key
    )
    half1,half2 = PolyMerger.half_datasets_from_isomorphous_mtzs(
        *parser.mtzinput, 
        anomalous=parser.anomalous, 
        dmin=None, 
        isigi_cutoff=parser.isigi_cutoff, 
        intensity_key=parser.intensity_key
    )

    merger.append_z_score_metadata(separate_files=parser.separate_files)
    half1.append_z_score_metadata(separate_files=parser.separate_files)
    half2.append_z_score_metadata(separate_files=parser.separate_files)

    merger.expand_harmonics(
        dmin=parser.dmin, 
        wavelength_key=parser.wavelength_key, 
        wavelength_range=parser.wavelength_range
    )
    half1.expand_harmonics(
        dmin=parser.dmin, 
        wavelength_key=parser.wavelength_key, 
        wavelength_range=parser.wavelength_range
    )
    half2.expand_harmonics(
        dmin=parser.dmin, 
        wavelength_key=parser.wavelength_key, 
        wavelength_range=parser.wavelength_range
    )
else:
    raise ValueError(f"Parser.type is {parser.type} but expected either 'mono' or 'poly'")

merger = main(merger, parser)
output_base = parser.output_base
if not parser.skip_xval:
    parser.output_base = output_base + '_half1'
    half1  = main(half1, parser)
    parser.output_base = output_base + '_half2'
    half2  = main(half2, parser)

if parser.embed:
    from matplotlib import pyplot as plt
    from IPython import embed
    embed(colors='Linux')
