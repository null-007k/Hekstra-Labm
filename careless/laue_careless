#!/usr/bin/env python

from os import environ
import pandas as pd
from tqdm import tqdm
from gemmi import UnitCell,SpaceGroup
import reciprocalspaceship as rs
import tensorflow as tf
import numpy as np

#Explicitly import everything until we figure out what should be in the __init__.py files
import careless
from careless.models.likelihoods.laue import *
from careless.models.scaling.nn import *
from careless.models.merging.variational import *
from careless.models.priors.empirical import *
from careless.models.priors.wilson import *
from careless.utils.io import *
from careless.utils.laue import *
from careless.parser import laue_parser as parser

#TODO: See if we still need this and decide if it should be a parser arg
max_nancount = 20

parser = parser.parse_args()
filenames = parser.reflection_filename


data = careless.utils.io.load_isomorphous_mtzs(*filenames)
data.compute_dHKL(inplace=True)


#Set a potentially different spacegroup for merging
if parser.space_group_number is not None:
    data.spacegroup = SpaceGroup(parser.space_group_number)

wavelength_key = parser.wavelength_key
if wavelength_key not in data:
    e = KeyError(f'Wavelength key "{parser.wavelength_key}" not found in Mtz files. Make sure to provide the correct wavelength key with "--wavelength-key".')
    raise e

metadata_keys = parser.metadata_keys
for key in metadata_keys:
    allowed_keys = list(data.keys()) + data.index.names
    if key not in allowed_keys:
        e = KeyError(f'Metadata key "{key}" not found in Mtz files.')
        raise e

output_filename = parser.output_mtz
if len(output_filename) < 5  or output_filename[-4:] != '.mtz':
    raise ValueError(f"{output_filename} is a valid output filename. Input filename must end in '.mtz'")
output_base = output_filename[:-4]

maxiter = parser.iterations
learning_rate = parser.learning_rate 

min_wavelength,max_wavelength = parser.wavelength_range
if min_wavelength is None:
    min_wavelength = data[parser.wavelength_key].min()
if max_wavelength is None:
    max_wavelength = data[parser.wavelength_key].max()

dmin = parser.dmin if parser.dmin is not None else data['dHKL'].min()
harmonic_dmin = 0.8*dmin #TODO: should this be a parser arg? it probably shouldn't be hardcoded?
data = data[data.dHKL >= dmin]
np.random.seed(parser.seed)
tf.random.set_seed(parser.seed)

if not parser.equate_batches:
    data['BATCH'] = data.groupby(['BATCH', 'file_id']).ngroup()

data['image_id'] = data.groupby(['BATCH', 'file_id']).ngroup()
#Set up half datasets for crossvalidation
data['half'] = (np.random.random(data['image_id'].max() + 1) > 0.5)[data['image_id']] 

cutoff = parser.isigi_cutoff
if cutoff is not None:
    data = data[data.I/data.SigI >= cutoff]

data = expand_harmonics(data, harmonic_dmin, wavelength_key)

#Remove harmonics outside the wavelength range
data = data[(data[wavelength_key] >= min_wavelength) & (data[wavelength_key] <= max_wavelength)]


#Remove systematic absences for merging space group
absent = rs.utils.hkl_is_absent(data.get_hkls(), data.spacegroup)
data = data[~absent]

#Move the hkls back to the asu for merging
H,_ = rs.utils.hkl_to_asu(data.get_hkls(), data.spacegroup)
data['H'],data['K'],data['L'] = H.T

data['epsilon'] = rs.utils.compute_structurefactor_multiplicity(data[['H', 'K', 'L']].to_numpy(), data.spacegroup)
if parser.anomalous:
    plus = (data['M/ISYM'] %2).astype(bool)
    data.loc[~plus, 'H'] = -data.loc[~plus, 'H'] 
    data.loc[~plus, 'K'] = -data.loc[~plus, 'K'] 
    data.loc[~plus, 'L'] = -data.loc[~plus, 'L'] 
data.label_centrics(inplace=True)
data['dHKL'] = data['dHKL']**-2.


if parser.merge_files:
    data['experiment_id'] = 0
else:
    data['experiment_id'] = data['file_id']

def train_model(df):
    df = df.reset_index()
    df['miller_id'] = df.groupby(['H', 'K', 'L', 'experiment_id']).ngroup()
    df['image_id'] = df.groupby(['BATCH', 'experiment_id']).ngroup()
    df['ray_id'] = df.groupby(['H_0','K_0', 'L_0']).ngroup()
    df['observation_id'] = df.groupby(['ray_id', 'image_id']).ngroup()

    if df.experiment_id.max() > 0:
        metadata = df[metadata_keys + ['experiment_id']].to_numpy().astype(np.float32) 
    else:
        metadata= df[metadata_keys].to_numpy().astype(np.float32) 

    metadata = (metadata - metadata.mean(0))/metadata.std(0)

    iobs = df.groupby('observation_id').first().I.to_numpy().astype(np.float32)
    sigiobs = df.groupby('observation_id').first().SigI.to_numpy().astype(np.float32)

    epsilons = df.groupby('miller_id').first()['epsilon'].to_numpy().astype(np.float32)
    centric = df.groupby('miller_id').first()['CENTRIC'].to_numpy().astype(np.float32)
    scaling_model = SequentialScaler(metadata, layers=20) #Gotta make this a parser argument

    harmonic_id = df['observation_id'].to_numpy().astype(np.int64)
    if parser.studentt:
        likelihood = StudentTLikelihood(iobs, sigiobs, harmonic_id, parser.studentt_dof)
    elif parser.laplace:
        likelihood = LaplaceLikelihood(iobs, sigiobs, harmonic_id)
    else:
        likelihood = NormalLikelihood(iobs, sigiobs, harmonic_id)

    if parser.laplace_prior:
        prior = LaplaceReferencePrior(Fobs, SigFobs)
    elif parser.normal_prior:
        prior = NormalReferencePrior(Fobs, SigFobs)
    else:
        prior = WilsonPrior(centric, epsilons)

    merger = VariationalMergingModel(
        df['miller_id'].to_numpy().astype(np.int64),
        [scaling_model],
        prior,
        likelihood,
    )

    optim = tf.keras.optimizers.Adam(learning_rate)#, clipvalue=.1)

    losses = []
    print(f"{'#'*80}")
    print(f'Optimizing model')
    print(f"{'#'*80}")
    loss_ = None
    chol_fails = 0
    nancount = 0
    for _ in tqdm(range(maxiter)):
        loss = merger.train_step(optim, s=parser.mc_iterations)
        losses.append(float(loss))
        if not tf.math.is_finite(loss):
            merger.rescue_variational_distributions()
            print(f"WARNING! Resetting stuck variational distributions!")
            nancount += 1
        else:
            nancount = 0
        loss_ = loss

        if nancount > max_nancount:
            print(f"WARNING! Optimization terminated due to too many failed gradient steps. Try decreasing the learning rate.")
            break

    results = pd.DataFrame()
    results['F'] = merger.surrogate_posterior.mean()
    results['SigF'] = merger.surrogate_posterior.stddev()
    results['dHKL'] = df.groupby('miller_id').first()['dHKL']
    results['H'] = df.groupby('miller_id')['H'].first()  
    results['K'] = df.groupby('miller_id')['K'].first()  
    results['L'] = df.groupby('miller_id')['L'].first()  
    results['experiment_id'] = df.groupby('miller_id')['experiment_id'].first()  
    keys = ['H', 'K', 'L', 'F', 'SigF', 'experiment_id']
    results = rs.DataSet(results[keys], spacegroup=data.spacegroup, cell=data.cell) 
    results.infer_mtz_dtypes(inplace=True)
    results.set_index(['H', 'K', 'L'], inplace=True)
    return merger,losses,results


print("Merging and refining full data set ...")
merger,losses,results = train_model(data)
np.save(output_filename[:-4] + "_losses.npy", losses)

for i,ds in results.groupby('experiment_id'):
    ds = ds.drop_duplicates() #TODO: This is probably a bug investigate this further. I am not sure why there are duplicates in here. It may have to do with harmonic deconvolution or the presence of multiple experiment_ids.
    rs.io.write_mtz(ds, output_filename.split('.')[0] + f'_{i}.mtz')

print("Merging and refining first half data set ...")
half1 = data[data['half']]
_,_,results1= train_model(half1)
rs.io.write_mtz(results1, output_filename[:-4] + '_half1.mtz')

print("Merging and refining second half data set ...")
half2 = data[~data['half']]
_,_,results2 = train_model(half2)
rs.io.write_mtz(results2, output_filename[:-4] + '_half2.mtz')
